---
title: "Practical Machine Learning Course Project"
author: "Doug McCaleb"
date: "Sunday, July 26, 2015"
output: html_document
---

### Executive Summary

In this class project for Practical Machine Learning with Jeff Leek, PhD, of Johns Hopkins Bloomberg School of Public Health, we derive the machine knowledge necessary for the machine to be able to interpret data to infer whether six people performed barbell lifts correctly or incorrectly.  The manner in which the physical exercises are performed is identified in the _classe_ variable, which reflects the category of each outcome alphanumerically as "A", "B", "C", "D", or "E".  

"Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes." "Six young [male] health participants [aged between 20 - 28 years]  were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)." (See http://groupware.les.inf.puc-rio.br/har#dataset)

Specifically, the goal of this project is to determine, without reference to the _classe_ variable in test data, whether the test participants performed the physical exercises correclty and, if they did not perform the exercises correctly, in what manner they performed the exercises incorrectly.

The data are taken from personal fitness devices such as the FitBit, the Nike FuelBand, and the Jawbone Up, all of which are able to produce signficant amounts of data. The data we will use for machine training comes from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv, and the data for testing the machine's knowledge comes from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv.  These data are provided by http://groupware.les.inf.puc-rio.br/har.

This document summarizes 1) how we get the data and prepare it for the exercise, 2) how we will cross-validate the data, 3) how we build the model and predict the out-of-sample error rate, 4) the validation of the model including its expected accuracy, 5) the application of the model to the final test data, and 6) the parsing of the results for submission to Coursera. 

### Housekeeping

```{r LibrariesSeed, echo=TRUE, warning=FALSE, message=FALSE,  }

library(RCurl)
library(caret)
library(rattle)
library(randomForest)
library(rpart.plot)
library(ggplot2)
library(lattice)
set.seed(123)
```

```{r EnableReadCSVURL, echo=FALSE, warning=FALSE, message=FALSE}
# From http://stackoverflow.com/questions/28997402/r-read-csv-from-url-error-in-knitr
# Enables direct read.csv of URL-sourced data from within Knitr execution.

read.csv.orig = read.csv

read.csv = function(file, ...) {
  if (is.character(file)) {
    if (grepl('^https://', file)) {
      data = getURL(file, ssl.verifypeer=0L, followlocation=1L)
      return (read.csv.orig(text=data, ...))  
    } else if (grepl('^http://', file)) {
      data = getURL(file)
      return (read.csv.orig(text=data, ...)) 
    } else {
      return (read.csv.orig(file, ...))
    }
  } else {
    return (read.csv.orig(file, ...))
  }
}
```

### Getting, Exploring, Cleaning, and Structuring the Data

Here we get the data from the sources, setting missing data along the way, and take a look at the structure.

```{r DownloadAndRead, echo=TRUE, message=TRUE}
trainingData <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", na.strings=c("NA", "#DIV/0!", ""))
testingData  <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",  na.strings=c("NA", "#DIV/0!", ""))
```

Let's take a look at the data . . .

```{r Structure, echo = TRUE, message=TRUE}
str(trainingData, list.len = 10)
str(testingData, list.len = 10)
table(trainingData$classe)
table(testingData$classe)
```

Some basic cleanup -- Based on our exploratory analysis, we will remove columns one through seven due to lack of relevance, and then remove columns that are NA.

```{r RemoveColumns,echo=TRUE, message=TRUE}
trainingData <- trainingData[,8:160]
testingData  <- testingData[,8:160]

isTrainingData  <- apply(!is.na(trainingData), 2, sum) > 19621  
trainingData <- trainingData[, isTrainingData]
dim(trainingData)

isTestingData  <- apply(!is.na(trainingData), 2, sum) > 19  
testingData <- testingData[, isTestingData]
dim(testingData)
```

We can see that we eliminated 100 columns from the trainingData data set.  

Before we start on the model, let's check on covariates in the trainingData to see if we can further simplify our data by eliminating net- or near- "zero covariates" that will not help us predict.

```{r NetZeroCovariates, echo = TRUE, message=TRUE}
NetZeroCovariateColumns <- nearZeroVar(trainingData)

NetZeroCovariateColumns
```

We can see from the fact that NetZeroCovariateColumns is zero that our prior data cleaning eliminated net zero covariates and that we should have clean predictors. 

### How We Will Cross-Validate

We break up the training data for cross-validation purposes -- We divide the training data 70% for buidling the model (training), and 30% for testing the model on the training data before applying the model to downloaded test data.  

```{r SplitTrainingData, echo = TRUE, message=TRUE}
dataForTraining <- createDataPartition(y = trainingData$classe, p = 0.7, list = FALSE)
trainingTrainingData <- trainingData[dataForTraining,]
testingTrainingData  <- trainingData[-dataForTraining,]
dim(trainingTrainingData)
dim(testingTrainingData)
```

This gives us a set of training data for actual training purposes having 13737 observations, or about 70% of the total training data, in the trainingTrainingData data set.  We also have 5885 observations of training data, in the testTrainingData data set, that we will use to test our model before using the model on the downloaded test data that is in the testingData data set.  

Later, we will also use K-fold cross-validation in the random forest alghorithm from the caret package to infer our model in the trainingTrainingData data set.

### Building the Model and Predicting Out-of-Sample Error

Due to its ability to handle large numbers of variables with unknown interactions, and its in-built cross-validation capability, we will first build the model on the training portion of the training data, the trainingTrainingData data set, using the randomForest alghorith with K-fold cross validation across five subsets of data, and see what kind of accuracy we get.

```{r BuildModel, echo = TRUE, message=TRUE}
trainingControl <- trainControl(method = "cv", 5)

modelFit <- train(classe ~ ., 
                  data = trainingTrainingData, 
                  method = "rf", 
                  trControl = trainingControl, 
                  ntree = 501)
modelFit

modelFit$finalModel
```

This looks promising, as the accuracy of the final model, mtry = 27, is 99.04%, and the out-of-sample (Out-of-Bag or OOB) error rate is predicted to be .66%

### Model Evaluation and Predicting Out-of-Sample Error Rate

We evaluate the model against the testingTrainingData data set first, producing a confusion matrix, and predicting its accuracy.

```{r EvaluateModel, echo = TRUE, message=TRUE}
testingTrainingDataPredictions <- predict(modelFit, testingTrainingData)

manualAccuracy <- sum(testingTrainingDataPredictions == testingTrainingData$classe)/length(testingTrainingDataPredictions)

manualAccuracy

manualOutOfSampleError <- 1 - manualAccuracy

manualOutOfSampleError

confusionMatrix(testingTrainingData$classe, testingTrainingDataPredictions)
```

Our test against the testingTrainingData predicts that the accuracy of the model will be 99.17%, with an out-of-sample error rate of .83%, calculated manually and using confusionMatrix.

We will use this for our first results submission. 

### Model Application to Test Data

Now we apply our model to the downloaded test data in the testingData data set.

```{r Results, echo = TRUE, message=TRUE}

results <- predict(modelFit, testingData)

results
```

### Uploading Results to Coursera

```{r UploadToCoursera, echo = TRUE, message=TRUE}
workingDirectory <- getwd()

setwd("F:/Prebuild Keep/Training/Coursera/8. Practical Machine Learning/CourseraSubmissionFiles")

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(results)

setwd(workingDirectory)
```

Our results turn out to be 100% accurate according to the Coursera results scoring page.  In this instance, use of random forest was a wise choice. 

### References

http://groupware.les.inf.puc-rio.br/har#dataset

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. 
Read more: http://groupware.les.inf.puc-rio.br/har#dataset#ixzz3gwqpfrXI

http://stackoverflow.com/questions/28997402/r-read-csv-from-url-error-in-knitr  for code to enable direct read.csv of URL-sourced data from within Knitr execution.
